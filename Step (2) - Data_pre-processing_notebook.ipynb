{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T05:44:34.235905Z",
     "start_time": "2022-03-13T05:44:25.792643Z"
    }
   },
   "outputs": [],
   "source": [
    "import arabic_reshaper                   # pip install arabic-reshaper\n",
    "from bidi.algorithm import get_display   # pip install python-bidi\n",
    "from emoji import get_emoji_regexp       # pip install emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "sns.set()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T05:44:34.264885Z",
     "start_time": "2022-03-13T05:44:34.238900Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_stopwords(file_path = './data/arabic_stopwords_list_nw.txt'):\n",
    "    with open(file_path, 'r', encoding = 'utf-8') as stopwords_file:\n",
    "        stopwords = [word.strip() for word in stopwords_file.readlines()]\n",
    "    # End file stream\n",
    "    return stopwords\n",
    "# End Func\n",
    "\n",
    "def format_arabic(text):\n",
    "    text = arabic_reshaper.reshape(text)\n",
    "    text = get_display(text)\n",
    "    return text\n",
    "# End Func\n",
    "\n",
    "def preprocess_text(text, remove_stopwords = False, stem = False, return_tokens = False):\n",
    "    # Remove mentions, hashtags or any english words and numbers \n",
    "    cleaning_regex_script = re.compile(pattern=r'(\\@\\w+|\\#\\w+|[A-Za-z0-9]+)')\n",
    "    text = cleaning_regex_script.sub('', text)\n",
    "\n",
    "    # Remove emojies\n",
    "    emoji_regex = get_emoji_regexp()\n",
    "    text = emoji_regex.sub('', text)\n",
    "\n",
    "    # Remove punctuations and some symbols\n",
    "    arabic_punctuations = '''`Ã·Ã—ÙªØ›<>_()*&^%][Ù€ØŒ/:\"ØŸ.,'{}~Â¦+|!â€â€¦â€œâ€“Ù€.'''\n",
    "    symbols = 'â¤â™¡â€â™©ï´¾ï´¿â†“ââ™¬'\n",
    "    puncts = arabic_punctuations + string.punctuation + symbols\n",
    "    text = text.translate(str.maketrans('', '', puncts))    \n",
    "\n",
    "    # Remove Arabic Digits\n",
    "    arabic_numbers_digits = r'[Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©]+'\n",
    "    text = re.sub(arabic_numbers_digits, '', text)\n",
    "\n",
    "    # Remove unnessary spaces\n",
    "    spaces_regex_script = re.compile(pattern=r'[\\s]{2,}')\n",
    "    text = spaces_regex_script.sub(' ', text).strip()\n",
    "\n",
    "    # Remove arabic diacritics\n",
    "    arabic_diacritics = r'[ÙÙÙÙŒÙ‹ÙÙ‘Ù€]'\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "\n",
    "    # Normalize the arabic text alpha\n",
    "    text = re.sub(\"[Ø¥Ø£Ø¢]\", \"Ø§\", text)\n",
    "    text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n",
    "    text = re.sub(\"Ø©\", \"Ù‡\", text)\n",
    "    text = re.sub(\"Ú¯\", \"Ùƒ\", text)\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    if remove_stopwords:\n",
    "        stopwords = load_stopwords()\n",
    "        tokens = [token for token in tokens if token not in stopwords and token.isalpha()]\n",
    "    # End if\n",
    "    \n",
    "    # Get words root using stemming\n",
    "    if stem:\n",
    "        stemmer = ISRIStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # End if\n",
    "    \n",
    "    preprocessed_text = tokens if return_tokens else ' '.join(tokens)\n",
    "    return preprocessed_text\n",
    "# End Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T05:44:34.294919Z",
     "start_time": "2022-03-13T05:44:34.269882Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = './data/dialects_data_full.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T05:44:39.194139Z",
     "start_time": "2022-03-13T05:44:34.297913Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path).drop(columns = ['id']).replace([''], np.nan).dropna().reset_index(drop = True)\n",
    "df['dialect'] = pd.Categorical(df['dialect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T05:56:04.409405Z",
     "start_time": "2022-03-13T05:44:39.197139Z"
    }
   },
   "outputs": [],
   "source": [
    "df['cleaned_text'] = df['text'].apply(preprocess_text).astype('U').copy()\n",
    "df['label'] = df['dialect'].values.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T05:56:04.450376Z",
     "start_time": "2022-03-13T05:56:04.412403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>dialect</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Nw8ieJUwaCAAreT Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠØ© .. ÙŠÙ†ØªÙØ¶ .. ÙŠØºÙŠØ± .</td>\n",
       "      <td>IQ</td>\n",
       "      <td>Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠÙ‡ ÙŠÙ†ØªÙØ¶ ÙŠØºÙŠØ±</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@7zNqXP0yrODdRjK ÙŠØ¹Ù†ÙŠ Ù‡Ø°Ø§ Ù…Ø­Ø³ÙˆØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø´Ø± .. Ø­...</td>\n",
       "      <td>IQ</td>\n",
       "      <td>ÙŠØ¹Ù†ÙŠ Ù‡Ø°Ø§ Ù…Ø­Ø³ÙˆØ¨ Ø¹Ù„ÙŠ Ø§Ù„Ø¨Ø´Ø± Ø­ÙŠÙˆÙ†Ù‡ ÙˆÙˆØ­Ø´ÙŠÙ‡ ÙˆØªØ·Ù„Ø¨ÙˆÙ† ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@KanaanRema Ù…Ø¨ÙŠÙ† Ù…Ù† ÙƒÙ„Ø§Ù…Ù‡ Ø®Ù„ÙŠØ¬ÙŠ</td>\n",
       "      <td>IQ</td>\n",
       "      <td>Ù…Ø¨ÙŠÙ† Ù…Ù† ÙƒÙ„Ø§Ù…Ù‡ Ø®Ù„ÙŠØ¬ÙŠ</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@HAIDER76128900 ÙŠØ³Ù„Ù…Ù„ÙŠ Ù…Ø±ÙˆØ±Ùƒ ÙˆØ±ÙˆØ­Ùƒ Ø§Ù„Ø­Ù„ÙˆÙ‡ğŸ’</td>\n",
       "      <td>IQ</td>\n",
       "      <td>ÙŠØ³Ù„Ù…Ù„ÙŠ Ù…Ø±ÙˆØ±Ùƒ ÙˆØ±ÙˆØ­Ùƒ Ø§Ù„Ø­Ù„ÙˆÙ‡</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@hmo2406 ÙˆÙŠÙ† Ù‡Ù„ Ø§Ù„ØºÙŠØ¨Ù‡  Ø§Ø® Ù…Ø­Ù…Ø¯ ğŸŒ¸ğŸŒº</td>\n",
       "      <td>IQ</td>\n",
       "      <td>ÙˆÙŠÙ† Ù‡Ù„ Ø§Ù„ØºÙŠØ¨Ù‡ Ø§Ø® Ù…Ø­Ù…Ø¯</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text dialect  \\\n",
       "0   @Nw8ieJUwaCAAreT Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠØ© .. ÙŠÙ†ØªÙØ¶ .. ÙŠØºÙŠØ± .      IQ   \n",
       "1  @7zNqXP0yrODdRjK ÙŠØ¹Ù†ÙŠ Ù‡Ø°Ø§ Ù…Ø­Ø³ÙˆØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø´Ø± .. Ø­...      IQ   \n",
       "2                    @KanaanRema Ù…Ø¨ÙŠÙ† Ù…Ù† ÙƒÙ„Ø§Ù…Ù‡ Ø®Ù„ÙŠØ¬ÙŠ      IQ   \n",
       "3         @HAIDER76128900 ÙŠØ³Ù„Ù…Ù„ÙŠ Ù…Ø±ÙˆØ±Ùƒ ÙˆØ±ÙˆØ­Ùƒ Ø§Ù„Ø­Ù„ÙˆÙ‡ğŸ’      IQ   \n",
       "4                 @hmo2406 ÙˆÙŠÙ† Ù‡Ù„ Ø§Ù„ØºÙŠØ¨Ù‡  Ø§Ø® Ù…Ø­Ù…Ø¯ ğŸŒ¸ğŸŒº      IQ   \n",
       "\n",
       "                                        cleaned_text  label  \n",
       "0                            Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠÙ‡ ÙŠÙ†ØªÙØ¶ ÙŠØºÙŠØ±      4  \n",
       "1  ÙŠØ¹Ù†ÙŠ Ù‡Ø°Ø§ Ù…Ø­Ø³ÙˆØ¨ Ø¹Ù„ÙŠ Ø§Ù„Ø¨Ø´Ø± Ø­ÙŠÙˆÙ†Ù‡ ÙˆÙˆØ­Ø´ÙŠÙ‡ ÙˆØªØ·Ù„Ø¨ÙˆÙ† ...      4  \n",
       "2                                Ù…Ø¨ÙŠÙ† Ù…Ù† ÙƒÙ„Ø§Ù…Ù‡ Ø®Ù„ÙŠØ¬ÙŠ      4  \n",
       "3                          ÙŠØ³Ù„Ù…Ù„ÙŠ Ù…Ø±ÙˆØ±Ùƒ ÙˆØ±ÙˆØ­Ùƒ Ø§Ù„Ø­Ù„ÙˆÙ‡      4  \n",
       "4                              ÙˆÙŠÙ† Ù‡Ù„ Ø§Ù„ØºÙŠØ¨Ù‡ Ø§Ø® Ù…Ø­Ù…Ø¯      4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T05:56:05.124971Z",
     "start_time": "2022-03-13T05:56:04.456375Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df, valid_test_df = train_test_split(df[['cleaned_text', 'label']], test_size = 0.2, stratify = df['label'])\n",
    "valid_df, test_df = train_test_split(valid_test_df, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T05:56:11.069643Z",
     "start_time": "2022-03-13T05:56:05.130968Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save Preprocessed Version\n",
    "df[['cleaned_text', 'dialect']].to_csv('./data/dataset_cleaned_version.csv', index = False)\n",
    "train_df.to_csv('./data/dataset_cleaned_version_train.csv', index = False)\n",
    "valid_df.to_csv('./data/dataset_cleaned_version_valid.csv', index = False)\n",
    "test_df.to_csv('./data/dataset_cleaned_version_test.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
